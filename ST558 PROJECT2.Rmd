---
title: "ST558 PROJECT2"
author: "Qiaozhi Bao"
date: "2020/10/6"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}
# Load all libraries
library(tidyverse)
library(ggplot2)
library(randomForest)
library(caret)
library(tree)
library(gbm)
library(corrplot)
library(e1071)
set.seed(1)
```
```{r}
# Read in data and removing the first two columns as they are not predictive variables.
news_pop <- read_csv('./OnlineNewsPopularity.csv') %>% select(-`url`,-`timedelta`)
```

```{r}
# First to see Monday data
Mon_data <- news_pop %>% select(!starts_with('weekday_is_'))
# Check if we have missing values, answer is 'No'
sum(is.na(Mon_data))
Mon_data
```
As there is no missing value in our Monday data, we will step to split data.
By using sample(), with 70% of the data goes to the training set (4,662 observations, Mon_train) and 30% goes to the test set (1,999 observations, Mon_test).
```{r}
# Split Monday data,70% for training set and 30% for test set
set.seed(1)
train <- sample(1:nrow(Mon_data),size = nrow(Mon_data)*0.7)
test <- dplyr::setdiff(1:nrow(Mon_data),train)
train_data <-Mon_data[train,]
test_data <- Mon_data[test,]

```
# Data Summarizations
## Response variable  
First I plot the histogram of the response variable `shares` and found it is a right-skewed distribution variable,then I performed log-transformation on `shares` and plot histogram too.
```{r}
# Histogram of the response variable
ggplot(data=train_data, aes(x=shares))+geom_histogram()
train_data$shares <- log(train_data$shares)
ggplot(data=train_data, aes(x=shares))+geom_histogram()+ xlab('Log(shares)')
```
## Predictor Variables  
I used the summary() function to calculate summary statistics for each of the quantitative variables in Mon_data.
```{r}
summary(train_data)
correlation1 <- cor(train_data[,c(1:10,52)])
corrplot(correlation1,type='upper',tl.pos = 'lt')
corrplot(correlation1,type='lower',method = 'number',add = T,diag = F,tl.pos = 'n')
correlation2 <- cor(train_data[,c(11:20,52)])
corrplot(correlation2,type='upper',tl.pos = 'lt')
corrplot(correlation2,type='lower',method = 'number',add = T,diag = F,tl.pos = 'n')
correlation3 <- cor(train_data[,c(21:30,52)])
corrplot(correlation3,type='upper',tl.pos = 'lt')
corrplot(correlation3,type='lower',method = 'number',add = T,diag = F,tl.pos = 'n')
correlation4 <- cor(train_data[,c(31:40,52)])
corrplot(correlation4,type='upper',tl.pos = 'lt')
corrplot(correlation4,type='lower',method = 'number',add = T,diag = F,tl.pos = 'n')
correlation5 <- cor(train_data[,c(41:51,52)])
corrplot(correlation5,type='upper',tl.pos = 'lt')
corrplot(correlation5,type='lower',method = 'number',add = T,diag = F,tl.pos = 'n')
```
From the correlation plot,I decided to remove some meaningless variables:`kw_min_min`,`kw_avg_min`,`kw_min_avg`,`is_weekend`,then we will get a new train set and test set
```{r}
train_data <- train_data %>% select(-c(is_weekend,kw_min_min, kw_avg_min, kw_min_avg))
test_data <- test_data %>% select(-c(is_weekend,kw_min_min, kw_avg_min, kw_min_avg))
train_data
test_data
```
## Tree based model chosen using leave one out cross validation
```{r}
bagged.tree <- train(shares ~.,data = train_data,method='treebag',
                     trControl = trainControl(method = 'LOOCV'), 
                    preProcess = c("center","scale"))

```

## Boosted tree model chosen using cross-validation
```{r}
# We will fit the model using repeated CV
boosted.tree <- train(shares ~.,data = train_data,method = 'gbm',
                      trControl = trainControl(method = 'repeatedcv', number=5,repeats =2),
                      preProcess = c("center","scale"),
                      verbose = FALSE)
```

```{r}
pred <- predict(boosted.tree,test_data)
summary(pred)
boosttbl <- table(data.frame(pred= predict(boosted.tree,test_data), true= test_data$shares))
# missclassification rate
1-sum(diag(boosttbl))/sum(boosttbl)
boostRMSE <- sqrt(mean((pred- test_data$shares)^2))
boostRMSE
```
```{r}


```
